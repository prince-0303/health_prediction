# -*- coding: utf-8 -*-
"""Healthcare_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Bxeu9PuLltGaj8K5rshMoFiMwdrsh5Z
"""

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('E:\VSCode projects\healthcare_prediction\Copy of healthcare_dataset.csv')

# df.info()                                                                                                   
# df.describe()

df['Billing Amount'] = pd.to_numeric(df['Billing Amount'], errors='coerce')
df['Billing Amount'].fillna(df['Billing Amount'].median(), inplace=True)

# df.isnull().sum()

# df.tail()

# df.shape

# df.dtypes

# df.nunique()

# df['Admission Type'].unique()

# df.duplicated().sum()

duplicate_rows = df[df.duplicated()].sort_values(by="Name")
# duplicate_rows

# df.drop_duplicates(inplace=True)

# df.head()

# sns.boxplot(x=df['Age'])
# plt.show()

df['Billing Amount'] = df['Billing Amount'].round(2)
# df.head()

df.drop(["Name", "Doctor", "Hospital", "Room Number",], axis = 1, inplace = True)
# df.head()

#admission_types = df['Admission Type'].value_counts()
#plt.pie(admission_types, labels=admission_types.index, autopct='%1.1f%%')
#plt.title('Admission Type Distribution')
#plt.show()

# gender_counts = df['Gender'].value_counts()
# percentages = gender_counts / gender_counts.sum() * 100
# print(percentages)
#
# plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%')
# plt.title('Gender')
# plt.show()

# Medication= df['Medication'].value_counts()
# plt.pie(Medication, labels=Medication.index, autopct='%1.1f%%')
# plt.title('Medication')
# plt.show()

# test_results = df['Test Results'].value_counts()
# plt.pie(test_results, labels=test_results.index, autopct='%1.1f%%')
# plt.title('Test Results')
# plt.show()

# import plotly.express as px


# fig = px.sunburst(df, path = ["Test Results", "Medication"])
# fig.update_traces(textinfo = "label + percent parent")
# fig.update_layout(title_text = "Test Results by Medication",
#                  titlefont = {'size' : 20, 'family' : 'Serif'},
#                  width = 600, height = 600)
# fig.show()

df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])
df['Days in hospital'] = (df['Discharge Date'] - df['Date of Admission']).dt.days
# df.head()

df.drop(["Date of Admission", "Discharge Date","Blood Type","Admission Type"], axis = 1, inplace = True)
# df.head()

# for column in ["Age", "Gender", "Medical Condition", "Insurance Provider", "Test Results", "Medication", ]:
#   plt.figure()
#   sns.histplot(df[column])
#   plt.title(f"Histplot of {column}")
  # plt.show()

# plt.figure(figsize=(10, 8))
# sns.heatmap(df.corr(numeric_only=True), annot=True,)

from sklearn.preprocessing import MinMaxScaler, StandardScaler

mmscaler = MinMaxScaler()
sscaler = StandardScaler()
for col in ['Age', 'Days in hospital']:
    df[col] = sscaler.fit_transform(df[[col]])


df['Billing Amount'] = mmscaler.fit_transform(df[[col]])
# df.head()

for col in df.columns:
  if df[col].dtype == 'object':
    print("Column name",col, "Values", df[col].nunique())

df = pd.get_dummies(df, columns=['Gender', 'Medical Condition', 'Insurance Provider', 'Test Results', 'Medication',], dtype=int)
# df.head()

# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# df['Admission Type'] = le.fit_transform(df['Admission Type'])
# df['Gender'] = le.fit_transform(df['Gender'])
# df['Test Results'] = le.fit_transform(df['Test Results'])
# df['Medication'] = le.fit_transform(df['Medication'])
# df['Blood Type'] = le.fit_transform(df['Blood Type'])
# df['Medical Condition'] = le.fit_transform(df['Medical Condition'])
# df.head()

# df = df[df['Billing Amount'] >= 0]

# negative_Billing_amount.shape

#from sklearn.preprocessing import StandardScaler

#scaler = StandardScaler()
#scaled_data = scaler.fit_transform(df)

# Created a new DataFrame with the scaled data
#df_scaled = pd.DataFrame(scaled_data, columns=df.columns)

# df_scaled.head()

#df_scaled.shape

from sklearn.model_selection import train_test_split

X = df.drop(['Billing Amount'], axis=1)
y = df['Billing Amount']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.head()

# from sklearn.linear_model import LinearRegression

# model = LinearRegression()
# model.fit(X_train, y_train)
# y_pred = model.predict(X_test)

# from sklearn.metrics import mean_squared_error, r2_score
# print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
# print("RMSE: ", np.sqrt(mean_squared_error(y_test,y_pred)))
# print("R-squared:", r2_score(y_test, y_pred))

# from sklearn.tree import DecisionTreeRegressor

# model = DecisionTreeRegressor()
# model.fit(X_train, y_train)
# y_pred = model.predict(X_test)

# print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
# print("R-squared:", r2_score(y_test, y_pred))

from sklearn.svm import SVR
model = SVR()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
# print("R-squared:", r2_score(y_test, y_pred))

from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVR

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly'],
    'epsilon': [0.1, 0.2, 0.3]
}

svr = SVR()

random_search = RandomizedSearchCV(estimator=svr, param_distributions=param_grid, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)

random_search.fit(X_train, y_train)

print("Best parameters found: ", random_search.best_params_)

best_model = random_search.best_estimator_

y_pred = best_model.predict(X_test)

print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R-squared:", r2_score(y_test, y_pred))

# !pip install scikit-optimize

# from skopt import BayesSearchCV
# from skopt.space import Real, Categorical

# # Define the search space for hyperparameters
# search_space = {
#     'C': Real(0.1, 100, prior='log-uniform'),
#     'gamma': Real(0.001, 1, prior='log-uniform'),
#     'kernel': Categorical(['linear', 'rbf', 'poly']),
#     'epsilon': Real(0.1, 0.3)
# }

# # Create the BayesSearchCV object
# bayes_search = BayesSearchCV(
#     estimator=SVR(),
#     search_spaces=search_space,
#     n_iter=10,
#     cv=5,
#     scoring='neg_mean_squared_error',
#     random_state=42
# )

# # Fit the model
# bayes_search.fit(X_train, y_train)

# # Print the best parameters found
# print("Best parameters found: ", bayes_search.best_params_)

# # Get the best model
# best_model = bayes_search.best_estimator_

# # Predict on the test set
# y_pred = best_model.predict(X_test)

# Evaluate the model
# print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
# print("R-squared:", r2_score(y_test, y_pred))

# from sklearn.model_selection import GridSearchCV

# param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}
# grid_search = GridSearchCV(SVR(), param_grid, cv=5)
# grid_search.fit(X_train, y_train)

# print("Best parameters:", grid_search.best_params_)
# print("Best score:", grid_search.best_score_)

# best_model = grid_search.best_estimator_
# y_pred = best_model.predict(X_test)

# print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
# print("R-squared:", r2_score(y_test, y_pred))

# Save the model to a file
with open('model.pkl', 'wb') as file:
    pickle.dump(best_model, file)
